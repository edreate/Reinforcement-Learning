{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the randomness so the demo is stable in class.\n",
    "# Comment these out if you want a different random world each run.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 1. GridWorld environment\n",
    "# ------------------------------\n",
    "class GridWorld:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.grid = np.zeros((size, size))  # 0 = normal cell (reward 0)\n",
    "        self._build_random_world()\n",
    "        self.state = self.start_state\n",
    "\n",
    "    def _build_random_world(self):\n",
    "        \"\"\"\n",
    "        Create a random goal and obstacle.\n",
    "        Start is fixed in the bottom-left corner (like your original example).\n",
    "        \"\"\"\n",
    "        self.start_state = (self.size - 1, 0)  # (row=3, col=0) in a 4x4 grid\n",
    "\n",
    "        all_states = [(i, j) for i in range(self.size) for j in range(self.size)]\n",
    "        possible = [s for s in all_states if s != self.start_state]\n",
    "\n",
    "        # Random goal (not at the start)\n",
    "        self.goal_state = random.choice(possible)\n",
    "        possible = [s for s in possible if s != self.goal_state]\n",
    "\n",
    "        # Random obstacle (not at start or goal)\n",
    "        self.obstacle_state = random.choice(possible)\n",
    "\n",
    "        # Fill grid with rewards\n",
    "        self.grid.fill(0.0)\n",
    "        self.grid[self.goal_state] = 1.0  # reward for goal\n",
    "        self.grid[self.obstacle_state] = -1.0  # penalty for obstacle\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Start a new episode from the start state.\"\"\"\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"Episode ends when we hit the goal or the obstacle.\"\"\"\n",
    "        return self.grid[state] == 1 or self.grid[state] == -1\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"Actions: 0 = up, 1 = right, 2 = down, 3 = left.\"\"\"\n",
    "        i, j = state\n",
    "        if action == 0:  # up\n",
    "            i = max(0, i - 1)\n",
    "        elif action == 1:  # right\n",
    "            j = min(self.size - 1, j + 1)\n",
    "        elif action == 2:  # down\n",
    "            i = min(self.size - 1, i + 1)\n",
    "        elif action == 3:  # left\n",
    "            j = max(0, j - 1)\n",
    "        return (i, j)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take one step in the environment.\"\"\"\n",
    "        next_state = self.get_next_state(self.state, action)\n",
    "        reward = self.grid[next_state]\n",
    "        self.state = next_state\n",
    "        done = self.is_terminal(next_state)\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0156eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2. Q-learning agent\n",
    "# ------------------------------\n",
    "class QLearningAgent:\n",
    "    def __init__(\n",
    "        self, state_shape, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1\n",
    "    ):\n",
    "        # Q[row, col, action] = value of taking that action in that state\n",
    "        self.q_table = np.zeros(state_shape + (4,))  # 4 actions\n",
    "        self.learning_rate = learning_rate  # α\n",
    "        self.discount_factor = discount_factor  # γ\n",
    "        self.exploration_rate = exploration_rate  # ε\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"ε-greedy: sometimes explore, otherwise exploit.\"\"\"\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            return random.randint(0, 3)  # Explore (random action)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_table[state]))  # Exploit (best known action)\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Tabular Q-learning update:\n",
    "        Q(s,a) ← Q(s,a) + α [ r + γ max_a' Q(s',a') - Q(s,a) ]\n",
    "        If next_state is terminal, we use just r as the target.\n",
    "        \"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            max_future_q = np.max(self.q_table[next_state])\n",
    "            target = reward + self.discount_factor * max_future_q\n",
    "\n",
    "        self.q_table[state][action] = current_q + self.learning_rate * (\n",
    "            target - current_q\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 3. Visualization helpers\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "def format_state_for_display(state):\n",
    "    \"\"\"\n",
    "    Convert internal (row, col) to display (x, y) with (0,0) at top-left:\n",
    "    x = col (horizontal), y = row (vertical).\n",
    "    \"\"\"\n",
    "    i, j = state\n",
    "    return f\"({j},{i})\"\n",
    "\n",
    "\n",
    "def plot_gridworld(env):\n",
    "    \"\"\"\n",
    "    Show the grid with:\n",
    "      - light blue = normal state\n",
    "      - red = obstacle (X)\n",
    "      - green = goal (G)\n",
    "      - coordinates (x, y) inside each cell, with (0,0) at top-left\n",
    "    Use this at the *start* to explain the state space.\n",
    "    \"\"\"\n",
    "    cmap = ListedColormap([\"lightblue\", \"red\", \"green\"])\n",
    "\n",
    "    viz_grid = np.zeros_like(env.grid)\n",
    "    viz_grid[env.grid == -1] = 1  # obstacle\n",
    "    viz_grid[env.grid == 1] = 2  # goal\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(viz_grid, cmap=cmap)\n",
    "\n",
    "    # Coordinate labels inside each cell\n",
    "    # x = column (j), y = row (i), (0,0) is top-left\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            plt.text(j, i, f\"({j},{i})\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    # Mark special cells (S, G, X) - positions unchanged, just markers\n",
    "    si, sj = env.start_state\n",
    "    plt.text(sj, si, \"S\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "    gi, gj = env.goal_state\n",
    "    plt.text(gj, gi, \"G\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "    oi, oj = env.obstacle_state\n",
    "    plt.text(oj, oi, \"X\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "\n",
    "    plt.xticks(range(env.size))\n",
    "    plt.yticks(range(env.size))\n",
    "    plt.title(\"Random GridWorld (with coordinates)\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_q_table(agent, env):\n",
    "    \"\"\"\n",
    "    Very explicit, block-by-block Q-table print.\n",
    "    Each block is one state; each line is one action.\n",
    "    Coordinates are shown as (x, y) with (0,0) at top-left:\n",
    "    x = col, y = row.\n",
    "    \"\"\"\n",
    "    action_names = [\"UP\", \"RIGHT\", \"DOWN\", \"LEFT\"]\n",
    "\n",
    "    print(\"\\nQ-table (one block per state):\")\n",
    "    print(\"For each state (x, y), we show the value of each possible action.\")\n",
    "    print(\"Higher numbers = better actions for that state (according to the agent).\\n\")\n",
    "\n",
    "    for i in range(env.size):  # row (y)\n",
    "        for j in range(env.size):  # col (x)\n",
    "            print(f\"State (x={j}, y={i})\")\n",
    "            for a_index, a_name in enumerate(action_names):\n",
    "                value = agent.q_table[i, j, a_index]\n",
    "                print(f\"  {a_name:>5}: {value:6.2f}\")\n",
    "            print()  # blank line between states\n",
    "    print()\n",
    "\n",
    "\n",
    "def plot_policy_on_grid(env, agent):\n",
    "    \"\"\"\n",
    "    Plot the learned policy on the grid itself.\n",
    "\n",
    "    - red   = obstacle (X)\n",
    "    - green = goal (G)\n",
    "    - light blue = other cells\n",
    "    - in each non-terminal cell we draw an arrow for the best action\n",
    "    - NO coordinates inside boxes here (kept clean for the policy view)\n",
    "    \"\"\"\n",
    "    cmap = ListedColormap([\"lightblue\", \"red\", \"green\"])\n",
    "\n",
    "    viz_grid = np.zeros_like(env.grid)\n",
    "    viz_grid[env.grid == -1] = 1  # obstacle\n",
    "    viz_grid[env.grid == 1] = 2  # goal\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(viz_grid, cmap=cmap)\n",
    "\n",
    "    arrow_symbols = [\"↑\", \"→\", \"↓\", \"←\"]\n",
    "\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            cell_reward = env.grid[i, j]\n",
    "            if cell_reward == 1:\n",
    "                # Goal\n",
    "                plt.text(j, i, \"G\", ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
    "            elif cell_reward == -1:\n",
    "                # Obstacle\n",
    "                plt.text(j, i, \"X\", ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
    "            else:\n",
    "                # Non-terminal: draw arrow for best action\n",
    "                best_action = int(np.argmax(agent.q_table[i, j]))\n",
    "                arrow = arrow_symbols[best_action]\n",
    "                plt.text(j, i, arrow, ha=\"center\", va=\"center\", fontsize=14)\n",
    "\n",
    "    # Mark the start state with a small 'S' near the top of its cell\n",
    "    si, sj = env.start_state\n",
    "    plt.text(sj, si, \"S\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "\n",
    "    plt.xticks(range(env.size))\n",
    "    plt.yticks(range(env.size))\n",
    "    plt.title(\"Learned policy on the grid (arrows = best action)\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_greedy_episode(env, agent, max_steps=20):\n",
    "    \"\"\"\n",
    "    Run one episode using the learned greedy policy (no exploration)\n",
    "    and print the sequence of states.\n",
    "    States are printed as (x, y) with (0,0) at top-left.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    path = [state]\n",
    "    steps = 0\n",
    "    reward = 0.0\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        action = int(np.argmax(agent.q_table[state]))\n",
    "        next_state, reward, done = env.step(action)\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    print(\"One greedy episode after training:\")\n",
    "    print(\" -> \".join(format_state_for_display(s) for s in path))\n",
    "    if done:\n",
    "        print(\"Episode finished with reward:\", reward)\n",
    "    else:\n",
    "        print(\"Did not reach terminal state within max steps. Last reward:\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9abb2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 4. Training loop\n",
    "# ------------------------------\n",
    "env = GridWorld(size=4)\n",
    "\n",
    "print(\"Grid rewards (1 = goal, -1 = obstacle, 0 = other):\")\n",
    "print(env.grid)\n",
    "plot_gridworld(env)  # Visualize the random world at the start\n",
    "\n",
    "agent = QLearningAgent(\n",
    "    state_shape=env.grid.shape,\n",
    "    learning_rate=0.1,  # α\n",
    "    discount_factor=0.9,  # γ\n",
    "    exploration_rate=0.1,  # ε\n",
    ")\n",
    "\n",
    "episodes = 1000  # Number of training episodes\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)  # choose action a\n",
    "        next_state, reward, done = env.step(action)  # take action, get r, s'\n",
    "        agent.update_q_value(state, action, reward, next_state, done)  # update Q(s,a)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 5. What did the agent learn?\n",
    "# ------------------------------\n",
    "print_q_table(agent, env)  # Text version, very explicit\n",
    "plot_policy_on_grid(env, agent)  # Visual policy on the grid\n",
    "run_greedy_episode(env, agent)  # Show one greedy path"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
